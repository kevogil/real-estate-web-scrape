{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f20d718-1f68-4567-b76d-b3b97c2603ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from random import randint\n",
    "from time import sleep, ctime\n",
    "\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from config import driver, username, password, host, port, database\n",
    "\n",
    "def init_browser():\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    return Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f55d1a01-697d-4af7-8fcd-5e3b9609da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cities to search\n",
    "cities = ['Los-Angeles_CA','New-York_NY', 'Chicago_IL', 'Houston_TX']\n",
    "\n",
    "cities = ['Los-Angeles_CA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "018d9ab2-cbad-455c-bcde-cf47c0775f37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 93.0.4577\n",
      "Get LATEST driver version for 93.0.4577\n",
      "Driver [C:\\Users\\kesam\\.wdm\\drivers\\chromedriver\\win32\\93.0.4577.63\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Start browser\n",
    "browser = init_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a23840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create blank lists/dictionaries to store attributes\n",
    "prices = []\n",
    "beds = []\n",
    "baths = []\n",
    "sizes = []\n",
    "addresses = []\n",
    "statuses = []\n",
    "detail_pages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be2a2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_indicators = {}\n",
    "types = []\n",
    "fees = []\n",
    "pricesqfts = []\n",
    "garages = []\n",
    "years = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdfe8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify listing attributes from result card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f37e7a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Los-Angeles_CA...Thu Sep 23 18:24:08 2021\n",
      "\n",
      "Scraping page 1 at Thu Sep 23 18:24:13 2021\n",
      "Scraping page 2 at Thu Sep 23 18:24:22 2021\n",
      "Scraping page 3 at Thu Sep 23 18:24:28 2021\n",
      "Scraping page 4 at Thu Sep 23 18:24:38 2021\n",
      "Scraping page 5 at Thu Sep 23 18:24:47 2021\n",
      "\n",
      "----------------------------Thu Sep 23 18:24:49 2021\n",
      "\n",
      "Scraping complete\n"
     ]
    }
   ],
   "source": [
    "# Loop through each city\n",
    "for city in cities:\n",
    "\n",
    "    print(f\"Searching {city}...{ctime()}\\n\")\n",
    "    \n",
    "    # Loop through each search result page\n",
    "    for i in range(1, 6, 1):\n",
    "        \n",
    "        # Set dynamic URL\n",
    "        url = f\"https://www.realtor.com/realestateandhomes-search/{city}/pg-{i}\"\n",
    "        browser.visit(url)\n",
    "        \n",
    "        print(f\"Scraping page {i} at {ctime()}\")\n",
    "        \n",
    "        # HTML object\n",
    "        html = browser.html\n",
    "        \n",
    "        # Parse HTML with Beautiful Soup\n",
    "        soup = bs(html, \"html.parser\")\n",
    "\n",
    "        # Identify all listings\n",
    "        listings = soup.find_all('li', attrs={\"data-testid\": \"result-card\"})\n",
    "\n",
    "        # Loop through each listing to identify attributes\n",
    "        for listing in listings:          \n",
    "            try:\n",
    "                price = listing.find('span', attrs={\"data-label\": \"pc-price\"}).text.strip('$')\n",
    "                prices.append(price)\n",
    "            except:\n",
    "                prices.append('No Info')\n",
    "            try:\n",
    "                bed = int(listing.find('li', attrs={\"data-label\": \"pc-meta-beds\"}).text.strip('bed'))\n",
    "                beds.append(bed)\n",
    "            except:\n",
    "                beds.append('No Info')\n",
    "            try:\n",
    "                bath = float(listing.find('li', attrs={\"data-label\": \"pc-meta-baths\"}).text.strip()[0])\n",
    "                baths.append(bath)\n",
    "            except:\n",
    "                baths.append('No Info')\n",
    "            try:\n",
    "                size = listing.find('li', attrs={\"data-label\": \"pc-meta-sqft\"}).text.strip('sqft')\n",
    "                sizes.append(size)\n",
    "            except:\n",
    "                sizes.append('No Info')\n",
    "            try:\n",
    "                address = listing.find('div', attrs={\"data-label\": \"pc-address\"}).text\n",
    "                addresses.append(address)\n",
    "            except:\n",
    "                addresses.append('No Info')\n",
    "            try:\n",
    "                status = listing.find('span', attrs={\"class\": \"jsx-3853574337 statusText\"}).text\n",
    "                statuses.append(status)\n",
    "            except:\n",
    "                statuses.append('No Info')\n",
    "\n",
    "            \n",
    "            # Identify URL to listing detail page\n",
    "            detail_page = listing.find('a').get('href')\n",
    "            \n",
    "            # Append to list\n",
    "            detail_pages.append(detail_page)\n",
    "            \n",
    "        # Generate random number between 2 to 10 seconds to wait before continuing loop\n",
    "        sleep(randint(2,10))\n",
    "            \n",
    "    print(f\"\\n----------------------------{ctime()}\\n\")\n",
    "    \n",
    "print('Scraping complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3e3e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify listing attributes from detail page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768dcebe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 42 listings found\n",
      "\n",
      "Scraping details from listing 1 of 42\n",
      "Scraping details from listing 2 of 42\n",
      "Scraping details from listing 3 of 42\n",
      "Scraping details from listing 4 of 42\n",
      "Scraping details from listing 5 of 42\n",
      "Scraping details from listing 6 of 42\n",
      "Scraping details from listing 7 of 42\n",
      "Scraping details from listing 8 of 42\n",
      "Scraping details from listing 9 of 42\n",
      "Scraping details from listing 10 of 42\n",
      "Scraping details from listing 11 of 42\n",
      "Scraping details from listing 12 of 42\n",
      "Scraping details from listing 13 of 42\n",
      "Scraping details from listing 14 of 42\n",
      "Scraping details from listing 15 of 42\n",
      "Scraping details from listing 16 of 42\n",
      "Scraping details from listing 17 of 42\n",
      "Scraping details from listing 18 of 42\n",
      "Scraping details from listing 19 of 42\n",
      "Scraping details from listing 20 of 42\n",
      "Scraping details from listing 21 of 42\n",
      "Scraping details from listing 22 of 42\n",
      "Scraping details from listing 23 of 42\n",
      "Scraping details from listing 24 of 42\n",
      "Scraping details from listing 25 of 42\n",
      "Scraping details from listing 26 of 42\n",
      "Scraping details from listing 27 of 42\n",
      "Scraping details from listing 28 of 42\n",
      "Scraping details from listing 29 of 42\n",
      "Scraping details from listing 30 of 42\n",
      "Scraping details from listing 31 of 42\n",
      "Scraping details from listing 32 of 42\n",
      "Scraping details from listing 33 of 42\n",
      "Scraping details from listing 34 of 42\n",
      "Scraping details from listing 35 of 42\n",
      "Scraping details from listing 36 of 42\n",
      "Scraping details from listing 37 of 42\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "num_page = len(detail_pages)\n",
    "print(f\"Total of {num_page} listings found\\n\")\n",
    "\n",
    "# Loop through each listing detail page\n",
    "for detail_page in detail_pages:\n",
    "    \n",
    "    print(f\"Scraping details from listing {i} of {num_page}\")\n",
    "    \n",
    "    # Navigate to each href\n",
    "    detail_url = f\"https://www.realtor.com{detail_page}\"\n",
    "    \n",
    "    browser.visit(detail_url)\n",
    "    \n",
    "    # HTML object\n",
    "    html = browser.html\n",
    "\n",
    "    # Parse HTML with Beautiful Soup\n",
    "    detail_soup = bs(html, \"html.parser\")\n",
    "\n",
    "    # Identify all listing-indicators\n",
    "    details = detail_soup.find('div', attrs={\"data-testid\": \"listing-indicator\"})\n",
    "    re_li = re.compile('rui*')\n",
    "    \n",
    "    try:\n",
    "        for x in details.find_all('li', re_li):\n",
    "            listing_indicators[x.find_all('div', attrs={'class': re_li})[0].text] = x.find_all('div', attrs={'class': re_li})[1].text\n",
    "            \n",
    "        try:\n",
    "            property_type = listing_indicators['Property Type']\n",
    "            types.append(property_type)\n",
    "        except:\n",
    "            types.append('No Info')\n",
    "        try:\n",
    "            hoa_fee = listing_indicators['HOA Fees'].strip('/mo')\n",
    "            hoa_fee = hoa_fee.strip('$')\n",
    "            fees.append(hoa_fee)\n",
    "        except:\n",
    "            fees.append('No Info')\n",
    "        try:\n",
    "            pricesqft = listing_indicators['Price per sqft'].strip('$')\n",
    "            pricesqfts.append(pricesqft)\n",
    "        except:\n",
    "            pricesqfts.append('No Info') \n",
    "        try:\n",
    "            garage = listing_indicators['Garage'].strip(' car')\n",
    "            garage = garage.strip(' cars')\n",
    "            garages.append(garage)\n",
    "        except:\n",
    "            garages.append('No Info')\n",
    "        try:\n",
    "            year = listing_indicators['Year Built']\n",
    "            years.append(year)\n",
    "        except:\n",
    "            years.append('No Info')\n",
    "\n",
    "    except:\n",
    "        types.append('No Info')\n",
    "        fees.append('No Info')\n",
    "        pricesqfts.append('No Info')\n",
    "        garages.append('No Info')\n",
    "        years.append('No Info')\n",
    "        \n",
    "        sleep(randint(2,15))\n",
    "    \n",
    "    i = i + 1\n",
    "    \n",
    "    # Generate random number between 2 to 10 seconds to wait before continuing loop\n",
    "    # sleep(randint(2,10))\n",
    "    \n",
    "print('\\nScraping complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf552c-014f-4337-966b-c8ff4d4b76b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add attributes to dataframe\n",
    "df = pd.DataFrame({'Address': addresses, 'Status': status, 'Property Type': types, 'Price': prices, 'Price per sqft': pricesqfts, 'HOA Fees': fees, 'Bed': beds, 'Bath': baths, 'Size': sizes, 'Garage': garages, 'Year Built': years})\n",
    "\n",
    "# Extract address into Street, City, State, Zip\n",
    "street_city = df['Address'].str.split(',', expand=True)\n",
    "street_city = street_city.rename(columns={0: 'Street', 1: 'City', 2: 'state_zip'})\n",
    "state_zip = street_city['state_zip'].str.split(' ', expand=True)\n",
    "state_zip = state_zip.rename(columns={1: 'State', 2: 'Zip'})\n",
    "street_city = street_city.drop(columns='state_zip')\n",
    "state_zip = state_zip.drop(columns=0)\n",
    "\n",
    "street_city.reset_index(drop=True, inplace=True)\n",
    "state_zip.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create merged_df\n",
    "address_df = pd.concat([street_city, state_zip], axis=1) \n",
    "\n",
    "merged_df = pd.concat([df, address_df], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d707862",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('listings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e548d-d552-4e6a-bf65-296e6afedcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da300b-0e6d-4880-a166-8ff4549764d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup database connection\n",
    "connection_string = f\"{driver}://{username}:{password}@{host}:{port}/{database}\"\n",
    "engine = create_engine(connection_string)\n",
    "connection = engine.connect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
