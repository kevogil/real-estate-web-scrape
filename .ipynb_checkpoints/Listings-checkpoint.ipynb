{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f20d718-1f68-4567-b76d-b3b97c2603ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup as bs\n",

    "def init_browser():\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    return Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f55d1a01-697d-4af7-8fcd-5e3b9609da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cities to search\n",
    "cities = ['Los-Angeles_CA','New-York_NY', 'Chicago_IL', 'Houston_TX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "018d9ab2-cbad-455c-bcde-cf47c0775f37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 93.0.4577\n",
      "Get LATEST driver version for 93.0.4577\n",
      "Driver [/Users/wolfey/.wdm/drivers/chromedriver/mac64/93.0.4577.63/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Start browser\n",
    "browser = init_browser()"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [],
   "source": [
    "# Create blank lists/dictionaries to store attributes\n",
    "prices = []\n",
    "beds = []\n",
    "baths = []\n",
    "sizes = []\n",
    "addresses = []\n",
    "statuses = []\n",
    "detail_pages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be2a2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_indicators = {}\n",
    "types = []\n",
    "fees = []\n",
    "pricesqfts = []\n",
    "garages = []\n",
    "years = []"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify listing attributes from result card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f37e7a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Los-Angeles_CA...\n",
      "\n",
      "Scraping page: 1\n",
      "----------------------------\n",
      "\n",
      "Searching New-York_NY...\n",
      "\n",
      "Scraping page: 1\n",
      "Scraping page: 2\n",
     ]
    }
   ],
   "source": [
    "# Loop through each city\n",
    "for city in cities:\n",
    "\n",
    "    print(f\"Searching {city}...\\n\")\n",
    "    \n",
    "    # Loop through each search result page\n",
    "    for i in range(1, 6, 1):\n",
    "        \n",
    "        # Set dynamic URL\n",
    "        url = f\"https://www.realtor.com/realestateandhomes-search/{city}/pg-{i}\"\n",
    "        browser.visit(url)\n",
    "        \n",
    "        print(f\"Scraping page {i}\")\n",
    "        \n",
    "        # HTML object\n",
    "        html = browser.html\n",
    "        \n",
    "        # Parse HTML with Beautiful Soup\n",
    "        soup = bs(html, \"html.parser\")\n",
    "\n",
    "        # Identify all listings\n",
    "        listings = soup.find_all('li', attrs={\"data-testid\": \"result-card\"})\n",
    "\n",
    "        # Loop through each listing to identify attributes\n",
    "        for listing in listings:          \n",
    "            try:\n",
    "                price = listing.find('span', attrs={\"data-label\": \"pc-price\"}).text.strip('$')\n",
    "                prices.append(price)\n",

    "            except:\n",
    "                prices.append('No Info')\n",
    "            try:\n",
    "                bed = int(listing.find('li', attrs={\"data-label\": \"pc-meta-beds\"}).text.strip('bed'))\n",
    "                beds.append(bed)\n",
    "            except:\n",
    "                beds.append('No Info')\n",
    "            try:\n",
    "                bath = float(listing.find('li', attrs={\"data-label\": \"pc-meta-baths\"}).text.strip()[0])\n",
    "                baths.append(bath)\n",
    "            except:\n",
    "                baths.append('No Info')\n",
    "            try:\n",
    "                size = listing.find('li', attrs={\"data-label\": \"pc-meta-sqft\"}).text.strip('sqft')\n",
    "                sizes.append(size)\n",
    "            except:\n",
    "                sizes.append('No Info')\n",
    "            try:\n",
    "                address = listing.find('div', attrs={\"data-label\": \"pc-address\"}).text\n",
    "                addresses.append(address)\n",
    "            except:\n",
    "                addresses.append('No Info')\n",
    "            try:\n",
    "                status = listing.find('span', attrs={\"class\": \"jsx-3853574337 statusText\"}).text\n",
    "                statuses.append(status)\n",
    "            except:\n",
    "                statuses.append('No Info')\n",
    "            # Identify URL to listing detail page\n",
    "            detail_page = listing.find('a').get('href')\n",
    "            \n",
    "            # Append to list\n",
    "            detail_pages.append(detail_page)\n",
    "            \n",
    "        # Generate random number between 2 to 10 seconds to wait before continuing loop\n",
    "        # sleep(randint(2,10))\n",
    "            \n",
    "    print(\"\\n----------------------------\\n\")\n",
    "    \n",
    "print('Scraping complete')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## Identify listing attributes from detail page"
   ]
  },
  {
   "cell_type": "code",
   ]
  },
  {
   "cell_type": "code",

   "id": "8ebf552c-014f-4337-966b-c8ff4d4b76b3",
   "metadata": {
    "scrolled": true
   },
    "\n",
    "# Extract address into Street, City, State, Zip\n",
    "street_city = df['Address'].str.split(',', expand=True)\n",
    "street_city = street_city.rename(columns={0: 'Street', 1: 'City', 2: 'state_zip'})\n",
    "state_zip = street_city['state_zip'].str.split(' ', expand=True)\n",
    "state_zip = state_zip.rename(columns={1: 'State', 2: 'Zip'})\n",
    "street_city = street_city.drop(columns='state_zip')\n",
    "state_zip = state_zip.drop(columns=0)\n",
    "\n",
    "street_city.reset_index(drop=True, inplace=True)\n",
    "state_zip.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create merged_df\n",
    "address_df = pd.concat([street_city, state_zip], axis=1) \n",
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('listings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
