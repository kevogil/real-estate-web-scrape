{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f20d718-1f68-4567-b76d-b3b97c2603ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "def init_browser():\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    return Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f55d1a01-697d-4af7-8fcd-5e3b9609da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cities to search\n",
    "cities = ['Los-Angeles_CA','New-York_NY', 'Chicago_IL', 'Houston_TX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "018d9ab2-cbad-455c-bcde-cf47c0775f37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 93.0.4577\n",
      "Get LATEST driver version for 93.0.4577\n",
      "Get LATEST driver version for 93.0.4577\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/93.0.4577.63/chromedriver_mac64.zip\n",
      "Driver has been saved in cache [/Users/kevingil/.wdm/drivers/chromedriver/mac64/93.0.4577.63]\n"
     ]
    }
   ],
   "source": [
    "# Start browser\n",
    "browser = init_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a23840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create blank lists/dictionaries to store attributes\n",
    "prices = []\n",
    "beds = []\n",
    "baths = []\n",
    "sizes = []\n",
    "addresses = []\n",
    "statuses = []\n",
    "detail_pages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be2a2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_indicators = {}\n",
    "types = []\n",
    "fees = []\n",
    "pricesqfts = []\n",
    "garages = []\n",
    "years = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdfe8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify listing attributes from result card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f37e7a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Los-Angeles_CA...\n",
      "\n",
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Searching New-York_NY...\n",
      "\n",
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Searching Chicago_IL...\n",
      "\n",
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Searching Houston_TX...\n",
      "\n",
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Scraping complete\n"
     ]
    }
   ],
   "source": [
    "# Loop through each city\n",
    "for city in cities:\n",
    "\n",
    "    print(f\"Searching {city}...\\n\")\n",
    "    \n",
    "    # Loop through each search result page\n",
    "    for i in range(1, 6, 1):\n",
    "        \n",
    "        # Set dynamic URL\n",
    "        url = f\"https://www.realtor.com/realestateandhomes-search/{city}/pg-{i}\"\n",
    "        browser.visit(url)\n",
    "        \n",
    "        print(f\"Scraping page {i}\")\n",
    "        \n",
    "        # HTML object\n",
    "        html = browser.html\n",
    "        \n",
    "        # Parse HTML with Beautiful Soup\n",
    "        soup = bs(html, \"html.parser\")\n",
    "\n",
    "        # Identify all listings\n",
    "        listings = soup.find_all('li', attrs={\"data-testid\": \"result-card\"})\n",
    "\n",
    "        # Loop through each listing to identify attributes\n",
    "        for listing in listings:          \n",
    "            try:\n",
    "                price = listing.find('span', attrs={\"data-label\": \"pc-price\"}).text.strip('$')\n",
    "                prices.append(price)\n",
    "            except:\n",
    "                prices.append('No Info')\n",
    "            try:\n",
    "                bed = int(listing.find('li', attrs={\"data-label\": \"pc-meta-beds\"}).text.strip('bed'))\n",
    "                beds.append(bed)\n",
    "            except:\n",
    "                beds.append('No Info')\n",
    "            try:\n",
    "                bath = float(listing.find('li', attrs={\"data-label\": \"pc-meta-baths\"}).text.strip()[0])\n",
    "                baths.append(bath)\n",
    "            except:\n",
    "                baths.append('No Info')\n",
    "            try:\n",
    "                size = listing.find('li', attrs={\"data-label\": \"pc-meta-sqft\"}).text.strip('sqft')\n",
    "                sizes.append(size)\n",
    "            except:\n",
    "                sizes.append('No Info')\n",
    "            try:\n",
    "                address = listing.find('div', attrs={\"data-label\": \"pc-address\"}).text\n",
    "                addresses.append(address)\n",
    "            except:\n",
    "                addresses.append('No Info')\n",
    "            try:\n",
    "                status = listing.find('span', attrs={\"class\": \"jsx-3853574337 statusText\"}).text\n",
    "                statuses.append(status)\n",
    "            except:\n",
    "                statuses.append('No Info')\n",
    "\n",
    "            \n",
    "            # Identify URL to listing detail page\n",
    "            detail_page = listing.find('a').get('href')\n",
    "            \n",
    "            # Append to list\n",
    "            detail_pages.append(detail_page)\n",
    "            \n",
    "        # Generate random number between 2 to 10 seconds to wait before continuing loop\n",
    "        # sleep(randint(2,10))\n",
    "            \n",
    "    print(\"\\n----------------------------\\n\")\n",
    "    \n",
    "print('Scraping complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify listing attributes from detail page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "768dcebe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 42 listings found\n",
      "\n",
      "Scraping details from listing 1 of 42\n",
      "Scraping details from listing 2 of 42\n",
      "Scraping details from listing 3 of 42\n",
      "Scraping details from listing 4 of 42\n",
      "Scraping details from listing 5 of 42\n",
      "Scraping details from listing 6 of 42\n",
      "Scraping details from listing 7 of 42\n",
      "Scraping details from listing 8 of 42\n",
      "Scraping details from listing 9 of 42\n",
      "Scraping details from listing 10 of 42\n",
      "Scraping details from listing 11 of 42\n",
      "Scraping details from listing 12 of 42\n",
      "Scraping details from listing 13 of 42\n",
      "Scraping details from listing 14 of 42\n",
      "Scraping details from listing 15 of 42\n",
      "Scraping details from listing 16 of 42\n",
      "Scraping details from listing 17 of 42\n",
      "Scraping details from listing 18 of 42\n",
      "Scraping details from listing 19 of 42\n",
      "Scraping details from listing 20 of 42\n",
      "Scraping details from listing 21 of 42\n",
      "Scraping details from listing 22 of 42\n",
      "Scraping details from listing 23 of 42\n",
      "Scraping details from listing 24 of 42\n",
      "Scraping details from listing 25 of 42\n",
      "Scraping details from listing 26 of 42\n",
      "Scraping details from listing 27 of 42\n",
      "Scraping details from listing 28 of 42\n",
      "Scraping details from listing 29 of 42\n",
      "Scraping details from listing 30 of 42\n",
      "Scraping details from listing 31 of 42\n",
      "Scraping details from listing 32 of 42\n",
      "Scraping details from listing 33 of 42\n",
      "Scraping details from listing 34 of 42\n",
      "Scraping details from listing 35 of 42\n",
      "Scraping details from listing 36 of 42\n",
      "Scraping details from listing 37 of 42\n",
      "Scraping details from listing 38 of 42\n",
      "Scraping details from listing 39 of 42\n",
      "Scraping details from listing 40 of 42\n",
      "Scraping details from listing 41 of 42\n",
      "Scraping details from listing 42 of 42\n",
      "\n",
      "Scraping complete\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "num_page = len(detail_pages)\n",
    "print(f\"Total of {num_page} listings found\\n\")\n",
    "\n",
    "# Loop through each listing detail page\n",
    "for detail_page in detail_pages:\n",
    "    \n",
    "    print(f\"Scraping details from listing {i} of {num_page}\")\n",
    "    \n",
    "    # Navigate to each href\n",
    "    detail_url = f\"https://www.realtor.com{detail_page}\"\n",
    "    \n",
    "    browser.visit(detail_url)\n",
    "    \n",
    "    # HTML object\n",
    "    html = browser.html\n",
    "\n",
    "    # Parse HTML with Beautiful Soup\n",
    "    detail_soup = bs(html, \"html.parser\")\n",
    "\n",
    "    # Identify all listing-indicators\n",
    "    details = detail_soup.find('div', attrs={\"data-testid\": \"listing-indicator\"})\n",
    "    re_li = re.compile('rui*')\n",
    "    \n",
    "    try:\n",
    "        for x in details.find_all('li', re_li):\n",
    "            listing_indicators[x.find_all('div', attrs={'class': re_li})[0].text] = x.find_all('div', attrs={'class': re_li})[1].text\n",
    "            \n",
    "        try:\n",
    "            property_type = listing_indicators['Property Type']\n",
    "            types.append(property_type)\n",
    "        except:\n",
    "            types.append('No Info')\n",
    "        try:\n",
    "            hoa_fee = listing_indicators['HOA Fees'].strip('/mo')\n",
    "            hoa_fee = hoa_fee.strip('$')\n",
    "            fees.append(hoa_fee)\n",
    "        except:\n",
    "            fees.append('No Info')\n",
    "        try:\n",
    "            pricesqft = listing_indicators['Price per sqft'].strip('$')\n",
    "            pricesqfts.append(pricesqft)\n",
    "        except:\n",
    "            pricesqfts.append('No Info') \n",
    "        try:\n",
    "            garage = listing_indicators['Garage'].strip(' car')\n",
    "            garage = garage.strip(' cars')\n",
    "            garages.append(garage)\n",
    "        except:\n",
    "            garages.append('No Info')\n",
    "        try:\n",
    "            year = listing_indicators['Year Built']\n",
    "            years.append(year)\n",
    "        except:\n",
    "            years.append('No Info')\n",
    "\n",
    "    except:\n",
    "        types.append('No Info')\n",
    "        fees.append('No Info')\n",
    "        pricesqfts.append('No Info')\n",
    "        garages.append('No Info')\n",
    "        years.append('No Info')\n",
    "        \n",
    "        sleep(15)\n",
    "    \n",
    "    i = i + 1\n",
    "    \n",
    "    # Generate random number between 2 to 10 seconds to wait before continuing loop\n",
    "    # sleep(randint(2,10))\n",
    "    \n",
    "print('\\nScraping complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20055b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1972',\n",
       " '2005',\n",
       " '1977',\n",
       " '2001',\n",
       " '1966',\n",
       " '1991',\n",
       " '1951',\n",
       " '1923',\n",
       " '1951',\n",
       " '1951',\n",
       " '1950',\n",
       " '1987',\n",
       " '1939',\n",
       " '1924',\n",
       " '1952',\n",
       " '1964',\n",
       " '1968',\n",
       " '1929',\n",
       " '1971',\n",
       " '1987',\n",
       " '1960',\n",
       " '1959',\n",
       " '1975',\n",
       " '1976',\n",
       " '1926',\n",
       " '1978',\n",
       " '2008',\n",
       " '1987',\n",
       " '1955',\n",
       " '1970',\n",
       " '2002',\n",
       " '1952',\n",
       " '1983',\n",
       " '1924',\n",
       " '1952',\n",
       " '1924',\n",
       " '1987',\n",
       " '1965',\n",
       " '1946',\n",
       " '1953',\n",
       " '1974',\n",
       " '2021']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf552c-014f-4337-966b-c8ff4d4b76b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add attributes to dataframe\n",
    "df = pd.DataFrame({'Address': addresses, 'Status': status, 'Property Type': types, 'Price': prices, 'Price per sqft': pricesqfts, 'HOA Fees': fees, 'Bed': beds, 'Bath': baths, 'Size': sizes, 'Garage': garages, 'Year Built': years})\n",
    "\n",
    "# Extract address into Street, City, State, Zip\n",
    "street_city = df['Address'].str.split(',', expand=True)\n",
    "street_city = street_city.rename(columns={0: 'Street', 1: 'City', 2: 'state_zip'})\n",
    "state_zip = street_city['state_zip'].str.split(' ', expand=True)\n",
    "state_zip = state_zip.rename(columns={1: 'State', 2: 'Zip'})\n",
    "street_city = street_city.drop(columns='state_zip')\n",
    "state_zip = state_zip.drop(columns=0)\n",
    "\n",
    "street_city.reset_index(drop=True, inplace=True)\n",
    "state_zip.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create merged_df\n",
    "address_df = pd.concat([street_city, state_zip], axis=1) \n",
    "\n",
    "merged_df = pd.concat([df, address_df], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d707862",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('listings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
